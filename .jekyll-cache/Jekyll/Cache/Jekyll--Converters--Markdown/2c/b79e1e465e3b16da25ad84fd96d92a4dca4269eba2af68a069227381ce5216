I"<details><summary><b>
Why video ads?
</b></summary>
Video advertisements (ads) or TV commercials have become an indispensable tool for marketing. Media advertising spending in the United States for the <a href="www.statista.com">year 2017 was about 206 Billion USD</a>. 
Companies not only invest heavily in advertising, but several com-panies generate revenue from ads. For example, the ad revenue for <a href="https://www.marketwatch.com/investing/stock/googl">Alphabet, Inc., rose from about 43 to 95 Billon USD from 2012â€“2017</a>
<p></p>
Considering the sheer number of ads being produced, it has become crucial to develop tools for a scalable and automatic analysis of ads. 
Automatic analysis of advertisements (ads) poses an interesting problem for learning multimodal representations. A promising direction of research is the development of deep neural network autoencoders to obtain inter-modal and intra-modal representations. 
<p></p>
</details>
<p></p>

<details><summary><b>
Why crossmodal learning?
</b></summary>
With lots of unlabeled data, we can exploit the co-occurring video and audio streams with their corresponding feature spaces to learn the audio-to-video and video-to-audio embeddings to capture the crossmodal relationships.
<p></p>
</details>
<p></p>

<blockquote>
  <p><strong>Proposed Method: Crossmodal Autoencoders</strong><br />
In this work, we propose a system to obtain segment-level unimodal and joint representations. These features are concatenated, and then averaged across the duration of an ad to obtain a single multimodal representation. The autoencoders are trained using segments generated by time-aligning frames between the audio and video modalities with forward and backward context.  <br />
<img src="https://docs.google.com/drawings/d/1AK6fnMbgtxycOnrWN8YQYv8m3205-7qH7Cp-c3B4vE8/export/png" alt="drawing" /></p>
</blockquote>

<p class="figcaption">Schematic of the overall system to learn crossmodal representations</p>

<blockquote>
  <p><strong>Dataset</strong><br />
In order to assess the multimodal representations, we consider the tasks of classifying attributes of an ad such as funny, exciting, sentiment and vertical (topic) in a publicly available dataset of 2,720 ads. <br />
For this purpose we train the segment-level autoencoders on a larger, unlabeled dataset of 9,740 ads, agnostic of the test set.</p>
</blockquote>

<h3 id="performance-analysis">Performance Analysis</h3>
<p><img src="/assets/img/projects/ads_perf.svg" alt="Ads performance analysis" /></p>

<p class="figcaption">Performance comparison of our method (multimodal AE) with other baselines for the ads dataset.</p>

<p>As shown above, multimodal AE features manage to capture the complementary information from audio and video modalities to be powerful for content analytic tasks such as the ones described above.</p>

<blockquote>
  <p><strong>Multimodal Dissociation</strong><br />
To understand why our embeddings learnt in an unsupervised fashion perform well, we examined what the individual and audio-to-video and video-to-audio representations capture, so we examined the correlation of the segment level features across different systems. The correlation pattern for one video ad is shown below:</p>
</blockquote>

<p><img src="/assets/img/projects/mm_disociate.svg" alt="Feature correlation" /></p>
:ET